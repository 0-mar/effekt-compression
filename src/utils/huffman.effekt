import stream
import io
import io/error
import src/utils/heap
import src/utils/ascii_dictionary
import bytearray
import src/utils/stream_io
import src/utils/huffman_tree

/// Huffman block structure:
/// +----------------------------------------------+----------------------------------------+------------------------------------------------+--------------------+
/// | 4 bytes - header length (in bytes)           | 4 bytes - number of encoded characters | Header                                         | Encoded content    |
/// |                                              |                                        | Last byte of header contains, how many bits    |                    |
/// |                                              |                                        | in the second last byte are part of the tree   |                    |
/// |                                              |                                        |                                                |                    |
/// +----------------------------------------------+----------------------------------------+------------------------------------------------+--------------------+

record HeaderStreamData(serializedTree: String, contentCharacterCount: Int, headerByteLength: Int, bitsInLastTreeByte: Int)
record HeaderData(root: HuffmanTree, contentCharacterCount: Int)
record ContentsData(table: AsciiDictionary[String], content: ByteArray)

///
/// Read chunk of the stream and make a character-frequency map (dictionary)
/// from it. Return the map, read chunk and its length.
/// 
def characterCompressionReader(): (AsciiDictionary[Int], Int, ByteArray) / read[Byte] = {
    var dict = emptyDict[Int]()

    val chunkSize = 32768 // 2 ** 15 bytes
    val buffer = bytearray::allocate(chunkSize)
    var offset = 0

    def go(): Unit = try {
        val readByte = do read[Byte]()

        buffer.unsafeSet(offset, readByte)
        offset = offset + 1

        val result = dict.get(readByte)
        result match {
            case Some(count) => dict.update(readByte, Some(count + 1))
            case None() => dict.update(readByte, Some(1))
        }

        // read only until we have space in the buffer
        if (offset < chunkSize) {
            go()
        } else {
            ()
        }
    } with stop {
        ()
    }

    go()
    (dict, offset, buffer)
}

///
/// Return tuple (header length in bytes, amount of bits in the second last byte used by the tree)
/// 
def get_header_metadata(serializedTree: String): (Int, Int) = {
    var bitLength = 0
    each(0, serializedTree.length) {
        i => 
            serializedTree.unsafeCharAt(i) match {
                case '0' => bitLength = bitLength + 1
                case '1' => bitLength = bitLength + 1
                case _ => bitLength = bitLength + 8
            }
    }

    var byteLength = bitLength / 8
    var leftovers = mod(bitLength, 8)
    leftovers match {
        case 0 => 
            byteLength = byteLength + 1
            leftovers = 8
        case _ => byteLength = byteLength + 2
    }
    (byteLength, leftovers)
}


def treeStream(serializedTree: String) = {
    each(0, serializedTree.length) {
        i => 
            serializedTree.unsafeCharAt(i) match {
                case '0' => do writeBit(false)
                case '1' => do writeBit(true)
                case char => do writeByte(char.toInt.toByte)
            }
    }
}

def writeHeaderStream(data: HeaderStreamData): Unit / emit[Byte] = {
    with streamWriter

    integerToBytesStream(data.headerByteLength)
    integerToBytesStream(data.contentCharacterCount)

    treeStream(data.serializedTree)
    do flush()  // ensure to fill the bit padding
    integerToBytesStream(data.bitsInLastTreeByte)
}

def writeCodeStream(code: String) = {
    each(0, code.length) {
        i => 
            code.unsafeCharAt(i) match {
                case '0' => do writeBit(false)
                case '1' => do writeBit(true)
                case _ => ()
            }
    }
}

def writeContentsStream(contentsData: ContentsData): Unit / emit[Byte] = {
    with streamWriter
    contentsData.content.foreach() {
        byte => 
            var value = contentsData.table.unsafeGet(byte)
            value match {
                case Some(code) => writeCodeStream(code)
                case None() => ()
            }
    }

    ()
}


def get_header_stream_data(headerData: HeaderData): HeaderStreamData = {
    val serializedTree = huffman_serialize(headerData.root)
    val (byteLength, bitsInLastByte) = get_header_metadata(serializedTree)

    HeaderStreamData(serializedTree, headerData.contentCharacterCount, byteLength, bitsInLastByte)
}

def writeCompressedStream(headerData: HeaderData, contentsData: ContentsData): Unit / emit[Byte] = {
    val headerStreamData = get_header_stream_data(headerData)
    writeHeaderStream(headerStreamData)
    writeContentsStream(contentsData)
}

def compressReaderIntoStream() = {
    loop { {l} =>
        println("Starting Huffman block compression...")
        val (frequencyMap, byteCount, readData) = characterCompressionReader()
        if (byteCount == 0) {
            // no more data to read => we are done
            println("No more data to read, terminating compression")
            l.break()
        } else {
            val root = build_tree(frequencyMap)
            val huffmanCodesTable = huffmanCodes(root)

            writeCompressedStream(HeaderData(root, byteCount), ContentsData(huffmanCodesTable, readData))

            println("Huffman block compression successful")
        }
    }
}